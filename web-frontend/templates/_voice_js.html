<script>
    const LIVEKIT_URL = "{{ livekit_url }}";
    let room = null;
    let connected = false;
    let audioContext = null;
    let analyser = null;
    let sourceNode = null;
    let animationFrame = null;
    let localMicStream = null;  // stored so we can switch back after agent stops
    let gainNode = null;        // boost mic input for visualizer

    // Create visualizer bars
    const visualizer = document.getElementById('visualizer');
    const NUM_BARS = 32;
    for (let i = 0; i < NUM_BARS; i++) {
        const bar = document.createElement('div');
        bar.className = 'bar';
        visualizer.appendChild(bar);
    }
    const bars = visualizer.querySelectorAll('.bar');

    const transcriptConsole = document.getElementById('transcriptConsole');
    // Map of segment id -> DOM element for in-place updates
    const segmentElements = {};

    function setStatus(state, text) {
        document.getElementById('statusDot').className = `status-dot ${state}`;
        document.getElementById('statusText').textContent = text;
    }

    function setAgentStatus(text) {
        document.getElementById('agentStatus').textContent = text;
    }

    async function toggleConnection() {
        if (connected) {
            await disconnect();
        } else {
            await connect();
        }
    }

    async function connect() {
        const btn = document.getElementById('connectBtn');
        btn.disabled = true;
        setStatus('connecting', 'Connecting...');

        try {
            const resp = await fetch('/api/token', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({})
            });
            const data = await resp.json();

            room = new LivekitClient.Room({
                audioCaptureDefaults: { autoGainControl: true, noiseSuppression: true },
                adaptiveStream: true,
            });

            room.on(LivekitClient.RoomEvent.TrackSubscribed, handleTrackSubscribed);
            room.on(LivekitClient.RoomEvent.TrackUnsubscribed, handleTrackUnsubscribed);
            room.on(LivekitClient.RoomEvent.TranscriptionReceived, handleTranscription);
            room.on(LivekitClient.RoomEvent.ParticipantConnected, (p) => {
                setAgentStatus(`Agent connected: ${p.identity}`);
            });
            room.on(LivekitClient.RoomEvent.ParticipantDisconnected, (p) => {
                setAgentStatus(`Agent disconnected: ${p.identity}`);
                stopVisualizer();
            });
            room.on(LivekitClient.RoomEvent.Disconnected, () => {
                handleDisconnect();
            });

            await room.connect(data.url, data.token);

            connected = true;
            setStatus('connected', `Connected to ${data.room}`);
            btn.textContent = 'Disconnect';
            btn.className = 'btn btn-disconnect';
            btn.disabled = false;
            setAgentStatus('Waiting for agent...');

            try {
                await Promise.race([
                    room.localParticipant.setMicrophoneEnabled(true),
                    new Promise((_, reject) => setTimeout(() => reject(new Error('Microphone setup timeout')), 10000)),
                ]);
                // Grab the local mic MediaStream for visualizer
                const micPub = room.localParticipant.getTrackPublication(LivekitClient.Track.Source.Microphone);
                if (micPub && micPub.track) {
                    // mediaStream may not exist on local tracks — build from mediaStreamTrack
                    localMicStream = micPub.track.mediaStream
                        || new MediaStream([micPub.track.mediaStreamTrack]);
                    startVisualizerFromStream(localMicStream, true);
                }
            } catch (micErr) {
                console.warn('Microphone unavailable:', micErr);
                setAgentStatus('Connected (mic unavailable). You can still receive agent audio.');
            }

        } catch (err) {
            console.error('Connection failed:', err);
            setStatus('error', 'Connection failed');
            setAgentStatus(err.message);
            btn.disabled = false;
        }
    }

    function handleTrackSubscribed(track, publication, participant) {
        if (track.kind === LivekitClient.Track.Kind.Audio) {
            // Attach for playback
            const element = track.attach();
            document.body.appendChild(element);

            // Switch visualizer to agent audio (no boost needed)
            startVisualizerFromStream(track.mediaStream, false);
            setAgentStatus('Agent is speaking...');
        }
    }

    function handleTrackUnsubscribed(track) {
        track.detach().forEach(el => el.remove());
        // Switch visualizer back to local mic if available
        if (localMicStream) {
            startVisualizerFromStream(localMicStream, true);
            setAgentStatus('Listening...');
        } else {
            stopVisualizer();
        }
    }

    function startVisualizerFromStream(mediaStream, isMic) {
        if (!mediaStream) return;
        if (!audioContext) {
            audioContext = new AudioContext();
        }
        audioContext.resume();

        // Disconnect old nodes
        if (sourceNode) { sourceNode.disconnect(); sourceNode = null; }
        if (gainNode) { gainNode.disconnect(); gainNode = null; }

        // Use MediaStream source — works for every new track/mic switch
        sourceNode = audioContext.createMediaStreamSource(mediaStream);
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 64;
        analyser.smoothingTimeConstant = 0.6;

        if (isMic) {
            // Mic input is much quieter than decoded agent audio — boost it
            // so the frequency data actually registers on the visualizer
            gainNode = audioContext.createGain();
            gainNode.gain.value = 4.0;
            analyser.minDecibels = -90;
            analyser.maxDecibels = -10;
            sourceNode.connect(gainNode);
            gainNode.connect(analyser);
        } else {
            sourceNode.connect(analyser);
        }
        // No need to connect to destination — playback is handled separately

        const dataArray = new Uint8Array(analyser.frequencyBinCount);

        if (animationFrame) {
            cancelAnimationFrame(animationFrame);
        }

        function animate() {
            analyser.getByteFrequencyData(dataArray);
            bars.forEach((bar, i) => {
                const value = dataArray[i] || 0;
                const height = Math.max(4, (value / 255) * 80);
                bar.style.height = `${height}px`;
            });
            animationFrame = requestAnimationFrame(animate);
        }
        animate();
    }

    function stopVisualizer() {
        if (animationFrame) {
            cancelAnimationFrame(animationFrame);
            animationFrame = null;
        }
        if (sourceNode) {
            sourceNode.disconnect();
            sourceNode = null;
        }
        if (gainNode) {
            gainNode.disconnect();
            gainNode = null;
        }
        bars.forEach(bar => {
            bar.style.height = '8px';
        });
    }

    // Transcript handling
    function handleTranscription(segments, participant, publication) {
        for (const segment of segments) {
            const segId = segment.id;
            const isFinal = segment.final;
            const text = segment.text;

            // Determine speaker
            const isUser = participant && room && participant.identity === room.localParticipant.identity;
            const speakerLabel = isUser ? 'You' : 'Agent';
            const speakerClass = isUser ? 'user' : 'agent';

            let lineEl = segmentElements[segId];
            if (!lineEl) {
                lineEl = document.createElement('div');
                lineEl.className = 'transcript-line';
                lineEl.innerHTML = `<span class="speaker ${speakerClass}">${speakerLabel}:</span><span class="text"></span>`;
                transcriptConsole.appendChild(lineEl);
                segmentElements[segId] = lineEl;
            }

            // Update text and interim styling
            lineEl.querySelector('.text').textContent = text;
            if (isFinal) {
                lineEl.classList.remove('interim');
            } else {
                lineEl.classList.add('interim');
            }

            // Auto-scroll
            transcriptConsole.scrollTop = transcriptConsole.scrollHeight;
        }
    }

    async function disconnect() {
        if (room) {
            await room.disconnect();
        }
        handleDisconnect();
    }

    function handleDisconnect() {
        connected = false;
        room = null;
        stopVisualizer();
        localMicStream = null;
        if (audioContext) {
            audioContext.close();
            audioContext = null;
            analyser = null;
        }
        setStatus('', 'Disconnected');
        setAgentStatus('');
        const btn = document.getElementById('connectBtn');
        btn.textContent = 'Connect';
        btn.className = 'btn btn-connect';
        btn.disabled = false;
        // Clear transcript state (keep display for review)
        Object.keys(segmentElements).forEach(k => delete segmentElements[k]);
    }
</script>
